2*exp(-2 * 0.3^2 * 10)
10!
factorial(10)
factorial(10)/(factorial(5)*factorial(5))
0.5^10
0.5^10 * 252
factorial(10)/(factorial(1)*factorial(9))
10 * (0.9^9 * 0.1)
0.1^10 + (0.1^9 * 0.9) * 10
2 * exp(-2 * 10 * 0.8^2)
12^5
2^5
256/32
1/6
pbeta(0.5, 1, 5)
qbeta(0.975, 8, 16)
qbeta(0.975, 8, 16, lower.tail = FALSE)
qbeta(0.025, 8, 16)
pbeta(0.35, 8, 16)
pbeta(0.35, 8, 21)
theta = range(0, 1, 0.01)
plot(theta, pbeta(theta, 1, 6))
pbeta(theta, 1, 6)
theta
range(0, 1, 0.01)
theta = seq(0, 1, 0.01)
plot(theta, pbeta(theta, 1, 6))
plot(theta, pbeta(theta, 1, 6), type='l')
plot(theta, dbeta(theta, 1, 6), type='l')
67/6
qbeta(0.05, 67, 6)
qgamma(0.05, 67, 6)
factor(5)
factorial(5)
factorial(4)*factorial(3)*factorial(3)/(
factorial(5)*factorial(2)*factorial(2)
)
factorial(4)*factorial(3)*factorial(3)
factorial(5)*factorial(2)*factorial(2)
factorial(4)*factorial(3)*factorial(2)/(
factorial(5)*factorial(2)*factorial(2)
)
factorial(14)*factorial(6)*factorial(9)/(
factorial(15)*factorial(5)*factorial(9)
)
factorial(3)
factorial(5)
gamma(5)
gamma(6)
gamma(4)*gamma(3)*gamma(2)/(
+     gamma(5)*gamma(2)*gamma(2)
+ )
gamma(4)*gamma(3)*gamma(2)/(
gamma(5)*gamma(2)*gamma(2)
)
gamma(14)*gamma(6)*gamma(9)/(
gamma(15)*gamma(5)*gamma(9)
)
(1/20)/(1/16)
1/4
0.25/0.8
20/16
20+12+15+8+13.5+25
93.5/6
6/93.5
pgamma(0.1, 6, 93.5)
30+16+8+114+60+4+23+30+105
qgamma(0.975, 9, 390)
f = function(){}
f = function(alpha, beta, y){return(
beta^alpha * alpla / (beta+y)^(alpha+1)
)}
seq(0, 200)
y = seq(0, 200)
f(9, 390, y)
f = function(alpha, beta, y){return(
beta^alpha * alpha / (beta+y)^(alpha+1)
)}
f(9, 390, y)
plot(y, f(9, 390, y), type='l')
y
f(9, 390, y)
plot(y, f(9, 390, y), type='l')
y=seq(0, 120)
y=seq(0, 120)
plot(y, f(9, 390, y), type='l')
20/16
(94.6+95.4+96.2+94.9+95.9)/5
5*95.4/0.25
1/24
(1908+400)/(20+4)
qnorm(0.975, 96.17, 0.042)
pnorm(100, 96.17, 0.042)
pnorm(50, 96.17, 0.042)
pnorm(96.17, 96.17, 0.042)
z <- rgamma(n=1000, shape=a, rate=b)
x <- 1/z
z <- rgamma(n=1000, shape=3, rate=200)
x <- 1/z
plot(x)
x
plot x
plot(x)
mean(x)
read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat <- read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
View(dat)
View(dat)
datF <- subset(dat, FM==1, select=1:2)
datF <- subset(dat, V3==1, select=1:2)
datM <- subset(dat, V3==2, select=1:2)
plot(datF)
plot(datM)
datF.lm=lm(datF$V1~datF$V2)
summary(datF.lm)
predict(datF.lm, data.frame(V1=260),interval="predict")
predict(datF.lm, data.frame(V1=260),interval="predict")
predict(datF.lm, interval="predict")
predict(datF.lm)
predict(datF.lm, data.frame(V1=260),interval="predict")
?predict
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
datF.lm=lm(V1~V2, data=datF)
summary(datF.lm)
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
predict(datF.lm, newdata = data.frame(V1=260, V2=0),interval="predict")
datF.lm=lm(V2~V1, data=datF)
summary(datF.lm)
predict(datF.lm, newdata = data.frame(V1=260),interval="predict")
dat$V3==1
dat[dat$V3==1]
dat(dat$V3==1])
dat(dat$V3==1)
dat$V1[dat$V3==1]
dat$V1[dat$V3==1] <- 0
dat=read.table(http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat=read.table("http://users.stat.ufl.edu/~winner/data/pgalpga2008.dat")
dat$V3[dat$V3==1] <- 0
dat$V3[dat$V3==2] <- 1
pairs(dat)
lm(V2~V1+V3, data = dat)
model = lm(V2~V1+V3, data = dat)
summary(model)
plot(fitted(model), residuals(model))
rgamma(10000, 5, 3)
theta = rgamma(10000, 5, 3)
result = theta / (1 - theta)
mean(result)
theta
mean(result)
result
mean(result)
result > 1
mean(result > 1)
theta = rbeta(10000, 5, 3)
result = theta / (1 - theta)
mean(result)
mean(result > 1)
n = rnorm(10000, 0, 1)
quantile(n, 0.3)
quantile(n)
sqrt(5.2/5000)
Q = matrix(c(0.0, 0.5,
0.5, 0.0,),
nrow=2, byrow=TRUE)
Q = matrix(c(0.0, 0.5,
0.5, 0.0),
nrow=2, byrow=TRUE)
Q = matrix(c(0.0, 1.0,
0.3, 0.7),
nrow=2, byrow=TRUE)
Q^4
Q %*% Q %*% Q %*% Q
Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q
Q %*% Q %*% Q
lg = function(mu, n, ybar) {
mu2 = mu^2
n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)
}
mh = function(n, ybar, n_iter, mu_init, cand_sd) {
## Random-Walk Metropolis-Hastings algorithm
## step 1, initialize
mu_out = numeric(n_iter)
accpt = 0
mu_now = mu_init
lg_now = lg(mu=mu_now, n=n, ybar=ybar)
## step 2, iterate
for (i in 1:n_iter) {
## step 2a
mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
## step 2b
lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
lalpha = lg_cand - lg_now # log of acceptance ratio
alpha = exp(lalpha)
## step 2c
u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
if (u < alpha) { # then accept the candidate
mu_now = mu_cand
accpt = accpt + 1 # to keep track of acceptance
lg_now = lg_cand
}
## collect results
mu_out[i] = mu_now # save this iteration's value of mu
}
## return a list of output
list(mu=mu_out, accpt=accpt/n_iter)
}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)
library("coda")
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.05)
post$accpt
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.9)
post$accpt
traceplot(as.mcmc(post$mu))
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=30.0, cand_sd=0.9)
post$accpt
traceplot(as.mcmc(post$mu))
post$mu_keep = post$mu[-c(1:100)] # discard the first 200 samples
plot(density(post$mu_keep, adjust=2.0), main="", xlim=c(-1.0, 3.0), xlab=expression(mu)) # plot density estimate of the posterior
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(ybar, 0, pch=19) # sample mean
curve(0.017*exp(lg(mu=x, n=n, ybar=ybar)), from=-1.0, to=3.0, add=TRUE, col="blue") # approximation to the true posterior in blue
density(post$mu_keep, adjust=2.0), main="", xlim=c(-1.0, 3.0), xlab=expression(mu)
density(post$mu_keep, adjust=2.0)
update_mu = function(n, ybar, sig2, mu_0, sig2_0) {
sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)
mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)
rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))
}
update_sig2 = function(n, y, mu, nu_0, beta_0) {
nu_1 = nu_0 + n / 2.0
sumsq = sum( (y - mu)^2 ) # vectorized
beta_1 = beta_0 + sumsq / 2.0
out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1) # rate for gamma is shape for inv-gamma
1.0 / out_gamma # reciprocal of a gamma random variable is distributed inv-gamma
}
gibbs = function(y, n_iter, init, prior) {
ybar = mean(y)
n = length(y)
## initialize
mu_out = numeric(n_iter)
sig2_out = numeric(n_iter)
mu_now = init$mu
## Gibbs sampler
for (i in 1:n_iter) {
sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)
mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)
sig2_out[i] = sig2_now
mu_out[i] = mu_now
}
cbind(mu=mu_out, sig2=sig2_out)
}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(as.mcmc(post))
summary(post)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
ybar = mean(y)
n = length(y)
## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(as.mcmc(post))
summary(post)
## prior
prior = list()
prior$mu_0 = 1.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
set.seed(53)
init = list()
init$mu = 0.0
post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
summary(post)
52 + 25 + 263
340 + 269.8
696+279+169.1+127.9+119.6+399+33+25
696+279+169.1+127.9+119.6+499+33+25
397+261
126.83 + 19.9 + 399 + 499 + 689
1948.6 + 261
2391.73+1948.6
2391.73+1948.6 / 2
(2391.73+1948.6) / 2
2391.73 - 2170.165
658 - 261
2391.73 - 261
1948.60 +261
696+279+169.1+127.9+119.6+499+33+25
696+279+169.1+127.9+119.6+449+33+25
2130.73 + 2159.6 + 2142 + 609.8
7509.93 - 7042.13
549+397+3485.6+178.83+2411.8+19.9
2130.73 + 2159.6
2142 + 609.8
467.8 - 2698
467.8 - 269.8
198
exp(-0.317466267006714)
0.3*2 + 0.45*3 + 0.25*0.5
0.2^2
0.3*(2 + 2^2) + 0.45*(3 + 3^2) + 0.25*(0.5 + 0.5^2) - (2.075)^2
43137 + 1731 + 12166 + 97 + 1217 + 1375 + 2939 + 2428 + 486
43137 + 1731 + 12166 + 97 + 1217 + 1375 + 2939 + 2428 + 486 + 16238
setwd('C:/Document/MixtureModel')
## Using mixture models for density estimation in the galaxies dataset
## Compare kernel density estimation, and estimates from mixtures of KK=6
## components obtained using both frequentist and Bayesian procedures
rm(list=ls())
### Loading data and setting up global variables
library(MASS)
library(MCMCpack)
data(galaxies)
KK = 6          # Based on the description of the dataset
x  = galaxies
n  = length(x)
set.seed(781209)
### First, compute the "Maximum Likelihood" density estimate associated with a location mixture of 6 Gaussian distributions using the EM algorithm
## Initialize the parameters
w     = rep(1,KK)/KK
mu    = rnorm(KK, mean(x), sd(x))
sigma = sd(x)/KK
epsilon = 0.000001
s       = 0
sw      = FALSE
KL      = -Inf
KL.out  = NULL
while(!sw){
## E step
v = array(0, dim=c(n,KK))
for(k in 1:KK){
v[,k] = log(w[k]) + dnorm(x, mu[k], sigma,log=TRUE)
}
for(i in 1:n){
v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
## M step
# Weights
w = apply(v,2,mean)
mu = rep(0, KK)
for(k in 1:KK){
for(i in 1:n){
mu[k]    = mu[k] + v[i,k]*x[i]
}
mu[k] = mu[k]/sum(v[,k])
}
# Standard deviations
sigma = 0
for(i in 1:n){
for(k in 1:KK){
sigma = sigma + v[i,k]*(x[i] - mu[k])^2
}
}
sigma = sqrt(sigma/sum(v))
##Check convergence
KLn = 0
for(i in 1:n){
for(k in 1:KK){
KLn = KLn + v[i,k]*(log(w[k]) + dnorm(x[i], mu[k], sigma, log=TRUE))
}
}
if(abs(KLn-KL)/abs(KLn)<epsilon){
sw=TRUE
}
KL = KLn
KL.out = c(KL.out, KL)
s = s + 1
print(paste(s, KLn))
}
xx  = seq(5000,37000,length=300)
nxx = length(xx)
density.EM = rep(0, nxx)
for(s in 1:nxx){
for(k in 1:KK){
density.EM[s] = density.EM[s] + w[k]*dnorm(xx[s], mu[k], sigma)
}
}
### Get a "Bayesian" kernel density estimator based on the same location mixture of 6 normals
## Priors set up using an "empirical Bayes" approach
aa  = rep(1,KK)
eta = mean(x)
tau = sqrt(var(x))
dd  = 2
qq  = var(x)/KK
## Initialize the parameters
w     = rep(1,KK)/KK
mu    = rnorm(KK, mean(x), sd(x))
sigma = sd(x)/KK
cc    = sample(1:KK, n, replace=T, prob=w)
## Number of iterations of the sampler
rrr   = 12000
burn  = 2000
## Storing the samples
cc.out    = array(0, dim=c(rrr, n))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK))
sigma.out = rep(0, rrr)
logpost   = rep(0, rrr)
for(s in 1:rrr){
# Sample the indicators
for(i in 1:n){
v = rep(0,KK)
for(k in 1:KK){
v[k] = log(w[k]) + dnorm(x[i], mu[k], sigma, log=TRUE)  #Compute the log of the weights
}
v = exp(v - max(v))/sum(exp(v - max(v)))
cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
# Sample the weights
w = as.vector(rdirichlet(1, aa + tabulate(cc, nbins=KK)))
# Sample the means
for(k in 1:KK){
nk    = sum(cc==k)
xsumk = sum(x[cc==k])
tau2.hat = 1/(nk/sigma^2 + 1/tau^2)
mu.hat  = tau2.hat*(xsumk/sigma^2 + eta/tau^2)
mu[k]   = rnorm(1, mu.hat, sqrt(tau2.hat))
}
# Sample the variances
dd.star = dd + n/2
qq.star = qq + sum((x - mu[cc])^2)/2
sigma = sqrt(1/rgamma(1, dd.star, qq.star))
# Store samples
cc.out[s,]   = cc
w.out[s,]    = w
mu.out[s,]   = mu
sigma.out[s] = sigma
for(i in 1:n){
logpost[s] = logpost[s] + log(w[cc[i]]) + dnorm(x[i], mu[cc[i]], sigma, log=TRUE)
}
logpost[s] = logpost[s] + log(ddirichlet(w, aa))
for(k in 1:KK){
logpost[s] = logpost[s] + dnorm(mu[k], eta, tau, log=TRUE)
}
logpost[s] = logpost[s] + dgamma(1/sigma^2, dd, qq, log=TRUE) - 4*log(sigma)
if(s/500==floor(s/500)){
print(paste("s =",s))
}
}
## Compute the samples of the density over a dense grid
density.mcmc = array(0, dim=c(rrr-burn,length(xx)))
for(s in 1:(rrr-burn)){
for(k in 1:KK){
density.mcmc[s,] = density.mcmc[s,] + w.out[s+burn,k]*dnorm(xx,mu.out[s+burn,k],sigma.out[s+burn])
}
}
density.mcmc.m = apply(density.mcmc , 2, mean)
## Plot Bayesian estimate with pointwise credible bands along with kernel density estimate and frequentist point estimate
colscale = c("black", "blue", "red")
yy = density(x)
density.mcmc.lq = apply(density.mcmc, 2, quantile, 0.025)
density.mcmc.uq = apply(density.mcmc, 2, quantile, 0.975)
plot(xx, density.mcmc.m, type="n",ylim=c(0,max(density.mcmc.uq)),xlab="Velocity", ylab="Density")
polygon(c(xx,rev(xx)), c(density.mcmc.lq, rev(density.mcmc.uq)), col="grey", border="grey")
lines(xx, density.mcmc.m, col=colscale[1], lwd=2)
lines(xx, density.EM, col=colscale[2], lty=2, lwd=2)
lines(yy, col=colscale[3], lty=3, lwd=2)
points(x, rep(0,n))
legend(27000, 0.00017, c("KDE","EM","MCMC"), col=colscale[c(3,2,1)], lty=c(3,2,1), lwd=2, bty="n")
